{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "feda6758-89d4-46b3-a96a-09c4c4c32e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  40 | Loss = 0.0070 | Acc = 100.0%\n",
      "Epoch  80 | Loss = 0.0018 | Acc = 100.0%\n",
      "Epoch 120 | Loss = 0.0011 | Acc = 100.0%\n",
      "Epoch 160 | Loss = 0.0007 | Acc = 100.0%\n",
      "Epoch 200 | Loss = 0.0005 | Acc = 100.0%\n",
      "\n",
      "Test predictions: [0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1) Generate tiny synthetic dataset\n",
    "# ----------------------------------------\n",
    "# Two classes in 2D:\n",
    "#  - class 0 around (-1, -1)\n",
    "#  - class 1 around (+1, +1)\n",
    "\n",
    "num_samples = 20\n",
    "\n",
    "class0 = torch.randn(num_samples, 2) * 0.3 + torch.tensor([-1.0, -1.0])\n",
    "class1 = torch.randn(num_samples, 2) * 0.3 + torch.tensor([1.0, 1.0])\n",
    "\n",
    "X = torch.cat([class0, class1], dim=0)\n",
    "y = torch.cat([torch.zeros(num_samples), torch.ones(num_samples)]).long()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2) Define a simple neural network\n",
    "# ----------------------------------------\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 2)    # 2 output classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = SimpleClassifier()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3) Training setup\n",
    "# ----------------------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4) Training loop\n",
    "# ----------------------------------------\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits = model(X)\n",
    "    loss = criterion(logits, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 40 == 0:\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        acc = (pred == y).float().mean().item()\n",
    "        print(f\"Epoch {epoch+1:3d} | Loss = {loss.item():.4f} | Acc = {acc*100:.1f}%\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 5) Test on new samples\n",
    "# ----------------------------------------\n",
    "test_points = torch.tensor([\n",
    "    [-1.2, -0.8],\n",
    "    [1.1,  0.9],\n",
    "    [0.0,  0.0]\n",
    "])\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(test_points)\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    print(\"\\nTest predictions:\", preds.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4879b58-ae59-41ca-8d74-49bb130e6ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/25 19:02:09 WARNING mlflow.models.signature: Failed to infer schema for inputs. Setting schema to `Schema([ColSpec(type=AnyType())]` as default. Note that MLflow doesn't validate data types during inference for AnyType. To see the full traceback, set logging level to DEBUG.\n",
      "2025/11/25 19:02:09 WARNING mlflow.models.signature: Failed to infer schema for outputs. Setting schema to `Schema([ColSpec(type=AnyType())]` as default. To see the full traceback, set logging level to DEBUG.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input example shape: (3, 2)\n",
      "Input example (first sample): [-0.66 -1.77]\n",
      "Model signature: inputs: \n",
      "  [Any (required)]\n",
      "outputs: \n",
      "  [Any (required)]\n",
      "params: \n",
      "  None\n",
      "\n",
      "✓ Input examples and signature prepared for MLflow logging\n"
     ]
    }
   ],
   "source": [
    "# Prepare input examples for model logging\n",
    "# Use a small sample of test data as input example\n",
    "input_example = np.array([\n",
    "    [-0.66, -1.77],\n",
    "    [-0.74, -1.42],\n",
    "    [-1.50, -0.92]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Convert to torch.float32 tensor\n",
    "input_example_tensor = torch.from_numpy(input_example).float()\n",
    "\n",
    "# Convert to numpy array for MLflow\n",
    "print(f\"Input example shape: {input_example.shape}\")\n",
    "print(f\"Input example (first sample): {input_example[0]}\")\n",
    "\n",
    "# Optionally, we can also create a model signature manually\n",
    "import mlflow.types.schema as schema\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Create model prediction for signature inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    example_output = model(X).numpy()\n",
    "\n",
    "# Infer signature from input and output\n",
    "signature = infer_signature(input_example, example_output)\n",
    "print(f\"Model signature: {signature}\")\n",
    "print(\"✓ Input examples and signature prepared for MLflow logging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81f72b00-0c72-41d8-8063-8ac8f344e327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_103/739978797.py:12: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n",
      "W1125 19:02:11.476000 103 .local/lib/python3.10/site-packages/torch/onnx/_internal/exporter/_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 11 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
      "W1125 19:02:11.964000 103 .local/lib/python3.10/site-packages/torch/onnx/_internal/exporter/_registration.py:107] torchvision is not installed. Skipping torchvision::nms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `SimpleClassifier([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `SimpleClassifier([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Model exported to ONNX format: mymodel.onnx\n",
      "ONNX model verification successful. Output shape: (5, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "def convert_to_onnx(model, input_size=(1,2), onnx_path=\"onnx_model.onnx\"):\n",
    "    \"\"\"Convert PyTorch model to ONNX format\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create dummy input for tracing\n",
    "    dummy_input = torch.randn(input_size, dtype=torch.float32)\n",
    "    \n",
    "    # Export to ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={\n",
    "            'input': {0: 'batch_size'},\n",
    "            'output': {0: 'batch_size'}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Model exported to ONNX format: {onnx_path}\")\n",
    "    return onnx_path\n",
    "\n",
    "def verify_onnx_model(onnx_path, test_data):\n",
    "    \"\"\"Verify ONNX model works correctly\"\"\"\n",
    "    # Load ONNX model\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    \n",
    "    # Create ONNX Runtime session\n",
    "    ort_session = ort.InferenceSession(onnx_path)\n",
    "    \n",
    "    # Test with a small batch\n",
    "    test_input = test_data[:5].numpy()  # Take first 5 samples\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: test_input}\n",
    "    ort_outputs = ort_session.run(None, ort_inputs)\n",
    "    \n",
    "    print(f\"ONNX model verification successful. Output shape: {ort_outputs[0].shape}\")\n",
    "    return True\n",
    "\n",
    "# Convert and verify ONNX model\n",
    "onnx_path = \"mymodel.onnx\"\n",
    "convert_to_onnx(model, input_size=(1, 2), onnx_path=onnx_path)\n",
    "verify_onnx_model(onnx_path, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d1d3fb2-2e9e-4537-9721-3961a2341d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/25 19:02:14 WARNING mlflow.models.signature: Failed to infer the model signature from the input example. Reason: ModuleNotFoundError(\"No module named 'scipy'\"). To see the full traceback, set the logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging PyTorch model to MLflow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/25 19:02:24 WARNING mlflow.models.model: Failed to validate serving input example {\n",
      "  \"inputs\": [\n",
      "    [\n",
      "      -0.6600000262260437,\n",
      "      -1.7699999809265137\n",
      "    ],\n",
      "    [\n",
      "      -0.7400000095367432,\n",
      "      -1.4199999570846558\n",
      "    ],\n",
      "    [\n",
      "      -1.5,\n",
      "      -0.9200000166893005\n",
      "    ]\n",
      "  ]\n",
      "}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\n",
      "Got error: mat1 and mat2 must have the same dtype, but got Double and Float\n",
      "Successfully registered model 'pytorch_classifier_notebook'.\n",
      "2025/11/25 19:02:26 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: pytorch_classifier_notebook, version 10\n",
      "Created version '10' of model 'pytorch_classifier_notebook'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging ONNX model to MLflow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'onnx_classifier_notebook'.\n",
      "2025/11/25 19:02:35 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: onnx_classifier_notebook, version 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models logged successfully to MLflow!\n",
      "MLflow Run ID: pl1o-3eod-7sl2-88uj\n",
      "✓ Models logged with input examples and signatures!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '7' of model 'onnx_classifier_notebook'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import onnxmltools\n",
    "from onnxmltools.convert.common.data_types import FloatTensorType\n",
    "\n",
    "REGISTERED_MODEL_NAME_ONNX = \"onnx_classifier_notebook\"\n",
    "with mlflow.start_run() as run:\n",
    "    # Log the PyTorch model to MLflow with input example and signature\n",
    "    print(\"Logging PyTorch model to MLflow...\")\n",
    "    mlflow.pytorch.log_model(\n",
    "        model, \n",
    "        \"classifier_pytorch\",\n",
    "        registered_model_name=\"pytorch_classifier_notebook\",\n",
    "        input_example=input_example\n",
    "    )\n",
    "    \n",
    "    # Log the ONNX model to MLflow with input example and signature  \n",
    "    print(\"Logging ONNX model to MLflow...\")\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    mlflow.onnx.log_model(\n",
    "        onnx_model,\n",
    "        \"classifier_onnx\",\n",
    "        registered_model_name=f\"{REGISTERED_MODEL_NAME_ONNX}\",\n",
    "        input_example=input_example\n",
    "    )\n",
    "    \n",
    "    print(\"Models logged successfully to MLflow!\")\n",
    "    print(f\"MLflow Run ID: {run.info.run_id}\")\n",
    "    print(\"✓ Models logged with input examples and signatures!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf9cc3-4395-4ec1-a720-fb37c84d8d51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
